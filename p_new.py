# -*- coding: utf-8 -*-
"""p new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-1SOMQuDFHc90vcQ3oCoICwA_Oa9VNtC
"""

# STEP 1: Install system dependencies and Python packages
# Run this cell first before running the main code

# # Install system dependencies
# !apt update
# !apt install tesseract-ocr -y
# !apt install libtesseract-dev -y

# # Install Python packages
# !pip install opendatasets
# !pip install pytesseract
# !pip install opencv-python
# !pip install scikit-image
# !pip install Pillow

# Test installations
try:
    import pytesseract
    print(f"‚úì Tesseract version: {pytesseract.get_tesseract_version()}")
except Exception as e:
    print(f"‚úó Tesseract installation issue: {e}")

try:
    import cv2
    print(f"‚úì OpenCV version: {cv2.__version__}")
except Exception as e:
    print(f"‚úó OpenCV installation issue: {e}")

try:
    import skimage
    print(f"‚úì scikit-image version: {skimage.__version__}")
except Exception as e:
    print(f"‚úó scikit-image installation issue: {e}")

try:
    import opendatasets
    print("‚úì opendatasets installed successfully")
except Exception as e:
    print(f"‚úó opendatasets installation issue: {e}")

print("\nAll packages installed! You can now run the main code.")

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import pickle
from scipy import ndimage
from skimage import measure, morphology
from skimage.filters import gaussian, unsharp_mask
from skimage.restoration import denoise_nl_means
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)

print("All libraries imported successfully!")

# # Install required packages for Colab (if not already installed)
# try:
#     import opendatasets as od
# except ImportError:
#     !pip install opendatasets
#     import opendatasets as od

# Download dataset
print("Downloading dataset...")
print("Note: You may need to provide your Kaggle credentials when prompted")
od.download("https://www.kaggle.com/datasets/debadityashome/iamsentences")
print("Dataset download completed!")

# !pip install transformers torch torchvision

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import pickle
from scipy import ndimage
from skimage import measure, morphology, filters
from skimage.filters import gaussian, unsharp_mask, threshold_otsu, threshold_local
from skimage.restoration import denoise_nl_means
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter, ImageOps
import warnings
warnings.filterwarnings('ignore')

# TrOCR imports
try:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    import torch
    TROCR_AVAILABLE = True
    print("TrOCR libraries loaded successfully!")
except ImportError as e:
    print(f"TrOCR not available: {e}")
    TROCR_AVAILABLE = False

class EnhancedTrOCRHandwrittenTextRecognizer:
    def __init__(self, dataset_path):
        self.dataset_path = dataset_path
        self.find_dataset_structure()

        if TROCR_AVAILABLE:
            print("Loading TrOCR models...")
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            # Load models
            self.processor_hw = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')
            self.model_hw = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten').to(self.device)

            self.processor_pr = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')
            self.model_pr = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed').to(self.device)

            print("TrOCR models loaded successfully!")

        self.tesseract_configs = {
            'handwriting_psm6': '--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?;:()-\'\"',
            'handwriting_psm7': '--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?;:()-\'\"',
            'lstm_only': '--oem 1 --psm 6 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?;:()-\'\"\n'
        }

        self.results = []
        self.accuracy_metrics = {}

    def find_dataset_structure(self):
        """Find dataset structure - KEEP UNCHANGED"""
        print(f"Exploring dataset structure in: {self.dataset_path}")

        possible_paths = [
            os.path.join(self.dataset_path, 'iamsentences'),
            os.path.join(self.dataset_path, 'iamsentences', 'sentences'),
            os.path.join(self.dataset_path, 'sentences'),
            self.dataset_path
        ]

        self.metadata_path = None
        self.images_path = None

        for path in possible_paths:
            if os.path.exists(path):
                contents = os.listdir(path)

                for item in contents:
                    item_path = os.path.join(path, item)
                    if os.path.isfile(item_path) and 'sentences' in item.lower():
                        self.metadata_path = item_path
                        break

                for item in contents:
                    item_path = os.path.join(path, item)
                    if os.path.isdir(item_path):
                        try:
                            dir_contents = os.listdir(item_path)
                            image_files = [f for f in dir_contents if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.tif'))]
                            if len(image_files) > 10:
                                self.images_path = item_path
                                break
                        except:
                            continue

                if self.metadata_path and self.images_path:
                    break

        if not self.metadata_path or not self.images_path:
            for root, dirs, files in os.walk(self.dataset_path):
                if not self.metadata_path:
                    for file in files:
                        if 'sentences' in file.lower():
                            self.metadata_path = os.path.join(root, file)
                            break

                if not self.images_path:
                    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.tif'))]
                    if len(image_files) > 10:
                        self.images_path = root
                        break

        print(f"Metadata: {self.metadata_path}")
        print(f"Images: {self.images_path}")

    def load_metadata(self):
        """Load metadata - KEEP UNCHANGED"""
        if not self.metadata_path or not os.path.exists(self.metadata_path):
            return [], []

        image_paths = []
        texts = []

        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            try:
                parts = line.split(' ')

                if len(parts) >= 9:
                    image_id = parts[0]
                    text = ' '.join(parts[8:])
                elif len(parts) >= 4:
                    image_id = parts[0]
                    text_start_idx = 1
                    for i in range(1, min(4, len(parts))):
                        if not parts[i].replace('.', '').replace('-', '').isdigit():
                            text_start_idx = i
                            break
                        text_start_idx = i + 1

                    text = ' '.join(parts[text_start_idx:])
                else:
                    continue

                text = text.replace('|', ' ').strip()
                text = ' '.join(text.split())

                words = text.split()
                if words and words[0].isdigit():
                    text = ' '.join(words[1:])

                if len(text) < 3 or len(text) > 100:
                    continue

                img_extensions = ['.png', '.jpg', '.jpeg', '.tiff', '.tif']
                img_path = None

                for ext in img_extensions:
                    potential_path = os.path.join(self.images_path, f"{image_id}{ext}")
                    if os.path.exists(potential_path):
                        img_path = potential_path
                        break

                if img_path:
                    image_paths.append(img_path)
                    texts.append(text.lower())

            except:
                continue

        print(f"Found {len(image_paths)} valid image-text pairs")
        return image_paths, texts

    def enhanced_preprocessing_for_trocr(self, img_path):
        """ENHANCED preprocessing for maximum recognition accuracy"""
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return None

        orig_h, orig_w = img.shape

        # Multi-stage noise reduction
        img = cv2.medianBlur(img, 3)
        img = cv2.bilateralFilter(img, 9, 80, 80)
        img = cv2.GaussianBlur(img, (3, 3), 0.5)

        # Advanced normalization
        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)

        # Contrast enhancement with histogram equalization
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        img = clahe.apply(img)

        # Multi-threshold approach for better character definition
        _, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        adaptive_mean = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 10)
        adaptive_gauss = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 10)

        # Smart combination preserving fine details
        combined = cv2.bitwise_and(otsu, adaptive_mean)
        combined = cv2.bitwise_or(combined, adaptive_gauss)

        # Morphological operations to connect broken characters
        kernel_conn = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel_conn)

        # Advanced connected component filtering
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(combined, connectivity=8)
        min_area = max(12, (orig_h * orig_w) // 6000)

        mask = np.zeros(combined.shape, dtype=np.uint8)
        for i in range(1, num_labels):
            area = stats[i, cv2.CC_STAT_AREA]
            height = stats[i, cv2.CC_STAT_HEIGHT]
            width = stats[i, cv2.CC_STAT_WIDTH]
            aspect_ratio = width / height if height > 0 else 0

            # Enhanced filtering for text components
            if (area >= min_area or
                (area >= 8 and height >= 4 and aspect_ratio < 5) or  # Letters
                (area >= 4 and height <= 8 and width <= 8)):        # Punctuation
                mask[labels == i] = 255

        # Skew correction
        mask = self.correct_skew_advanced(mask)

        # Optimal resizing for TrOCR with aspect ratio preservation
        height, width = mask.shape
        target_height = 160
        scale_factor = target_height / height
        new_width = int(width * scale_factor)

        mask = cv2.resize(mask, (new_width, target_height), interpolation=cv2.INTER_CUBIC)

        # Smart padding
        pad_h, pad_w = 30, 50
        mask = cv2.copyMakeBorder(mask, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_CONSTANT, value=255)

        # Ensure proper polarity
        if np.mean(mask) < 127:
            mask = cv2.bitwise_not(mask)

        return mask

    def correct_skew_advanced(self, img):
        """Advanced skew correction with improved accuracy"""
        try:
            # Multi-scale edge detection
            edges1 = cv2.Canny(img, 30, 100, apertureSize=3)
            edges2 = cv2.Canny(img, 50, 150, apertureSize=3)
            edges = cv2.bitwise_or(edges1, edges2)

            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=30, maxLineGap=10)

            if lines is not None:
                angles = []
                for line in lines:
                    x1, y1, x2, y2 = line[0]
                    if abs(x2 - x1) > 10:  # Avoid vertical lines
                        angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
                        if -15 < angle < 15:
                            angles.append(angle)

                if len(angles) > 5:
                    # Use median for robustness
                    median_angle = np.median(angles)
                    if abs(median_angle) > 0.3:
                        (h, w) = img.shape[:2]
                        center = (w // 2, h // 2)
                        M = cv2.getRotationMatrix2D(center, median_angle, 1.0)
                        img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

            return img
        except:
            return img

    def extract_text_with_trocr(self, img):
        """ENHANCED TrOCR extraction with ensemble approach"""
        if img is None or not TROCR_AVAILABLE:
            return ""

        try:
            # Convert to PIL with enhanced preprocessing
            pil_img = Image.fromarray(img).convert('RGB')

            # Multiple enhancement strategies
            results = []

            # Strategy 1: Enhanced contrast + sharpening
            enhancer = ImageEnhance.Contrast(pil_img)
            enhanced1 = enhancer.enhance(1.3)
            enhancer = ImageEnhance.Sharpness(enhanced1)
            enhanced1 = enhancer.enhance(1.5)

            # Strategy 2: Brightness adjustment + moderate sharpening
            enhancer = ImageEnhance.Brightness(pil_img)
            enhanced2 = enhancer.enhance(1.1)
            enhancer = ImageEnhance.Sharpness(enhanced2)
            enhanced2 = enhancer.enhance(1.2)

            # Strategy 3: Original with minimal enhancement
            enhancer = ImageEnhance.Contrast(pil_img)
            enhanced3 = enhancer.enhance(1.1)

            strategies = [
                ('enhanced_contrast', enhanced1),
                ('enhanced_brightness', enhanced2),
                ('minimal_enhance', enhanced3)
            ]

            for strategy_name, enhanced_img in strategies:
                # Handwritten model - multiple beam sizes
                for num_beams in [4, 6, 8]:
                    try:
                        pixel_values = self.processor_hw(enhanced_img, return_tensors="pt").pixel_values.to(self.device)
                        with torch.no_grad():
                            generated_ids = self.model_hw.generate(
                                pixel_values,
                                max_length=80,
                                num_beams=num_beams,
                                early_stopping=True,
                                do_sample=False,
                                length_penalty=0.9,
                                repetition_penalty=1.1
                            )

                        text = self.processor_hw.batch_decode(generated_ids, skip_special_tokens=True)[0]
                        if text.strip():
                            confidence = self.calculate_enhanced_confidence(text, enhanced_img, strategy_name, num_beams)
                            results.append({
                                'text': text,
                                'confidence': confidence,
                                'model': f'hw_{strategy_name}_beam{num_beams}'
                            })
                    except Exception as e:
                        continue

            # Printed model as additional option
            try:
                pixel_values = self.processor_pr(enhanced1, return_tensors="pt").pixel_values.to(self.device)
                with torch.no_grad():
                    generated_ids = self.model_pr.generate(
                        pixel_values,
                        max_length=60,
                        num_beams=5,
                        early_stopping=True
                    )
                text_pr = self.processor_pr.batch_decode(generated_ids, skip_special_tokens=True)[0]
                if text_pr.strip():
                    confidence_pr = self.calculate_enhanced_confidence(text_pr, enhanced1, 'printed', 5) * 0.85
                    results.append({
                        'text': text_pr,
                        'confidence': confidence_pr,
                        'model': 'printed'
                    })
            except:
                pass

            # Return best result or ensemble if close
            if results:
                results.sort(key=lambda x: x['confidence'], reverse=True)

                # If top results are close, use ensemble
                if len(results) > 1 and results[1]['confidence'] > results[0]['confidence'] * 0.9:
                    ensemble_text = self.create_ensemble_result([r['text'] for r in results[:3]])
                    return self.post_process_trocr_text(ensemble_text)
                else:
                    return self.post_process_trocr_text(results[0]['text'])

            return ""

        except Exception as e:
            print(f"TrOCR extraction error: {e}")
            return ""

    def calculate_enhanced_confidence(self, text, image, strategy, beam_size):
        """Enhanced confidence calculation with multiple factors"""
        if not text:
            return 0

        text_clean = text.strip()
        char_count = len(text_clean)
        words = text_clean.split()

        # Length scoring (optimal 8-60 chars)
        if char_count < 5:
            length_score = char_count / 5 * 0.5
        elif char_count <= 60:
            length_score = 1.0
        else:
            length_score = max(0.2, 1.0 - (char_count - 60) / 100)

        # Character composition
        alpha_chars = sum(1 for c in text_clean if c.isalpha())
        space_chars = text_clean.count(' ')
        punct_chars = sum(1 for c in text_clean if c in '.,!?;:()-\'\"')

        alpha_ratio = alpha_chars / char_count if char_count > 0 else 0
        space_ratio = space_chars / char_count if char_count > 0 else 0
        punct_ratio = punct_chars / char_count if char_count > 0 else 0

        composition_score = (
            alpha_ratio * 0.8 +
            min(space_ratio * 5, 0.15) +
            min(punct_ratio * 12, 0.05)
        )

        # Word structure analysis
        if words:
            avg_word_len = sum(len(w.strip('.,!?;:()-\'\"')) for w in words) / len(words)
            word_len_score = max(0.3, 1.0 - abs(avg_word_len - 5) / 8)

            # Check for realistic words
            realistic_words = 0
            for word in words:
                clean_word = word.strip('.,!?;:()-\'\"').lower()
                if len(clean_word) >= 2:
                    vowels = sum(1 for c in clean_word if c in 'aeiouAEIOU')
                    consonants = len(clean_word) - vowels
                    if vowels > 0 and consonants > 0:
                        realistic_words += 1

            word_realism_score = realistic_words / len(words) if words else 0
        else:
            word_len_score = 0.4
            word_realism_score = 0

        # Strategy and beam bonuses
        strategy_bonus = 1.0
        if 'enhanced_contrast' in strategy:
            strategy_bonus = 1.05
        elif 'enhanced_brightness' in strategy:
            strategy_bonus = 1.03

        beam_bonus = min(1.1, 1.0 + (beam_size - 4) * 0.02)

        # Combine all factors
        final_confidence = (
            length_score * 0.2 +
            composition_score * 0.4 +
            word_len_score * 0.2 +
            word_realism_score * 0.2
        ) * strategy_bonus * beam_bonus

        return min(1.0, final_confidence)

    def create_ensemble_result(self, texts):
        """Create ensemble result from multiple predictions"""
        if not texts:
            return ""

        # Simple voting: take most common words at each position
        word_lists = [text.split() for text in texts if text.strip()]
        if not word_lists:
            return texts[0] if texts else ""

        max_len = max(len(words) for words in word_lists)
        ensemble_words = []

        for i in range(max_len):
            word_candidates = []
            for word_list in word_lists:
                if i < len(word_list):
                    word_candidates.append(word_list[i])

            if word_candidates:
                # Use most common word, or longest if tie
                word_counts = Counter(word_candidates)
                most_common = word_counts.most_common(1)[0]
                if word_counts[most_common[0]] > 1:
                    ensemble_words.append(most_common[0])
                else:
                    # Use longest word among candidates
                    ensemble_words.append(max(word_candidates, key=len))

        return ' '.join(ensemble_words)

    def post_process_trocr_text(self, text):
        """Enhanced post-processing for better accuracy"""
        if not text:
            return ""

        text = text.strip()

        # Fix common spacing issues
        text = ' '.join(text.split())  # Normalize spaces
        text = text.replace(' .', '.')
        text = text.replace(' ,', ',')
        text = text.replace(' !', '!')
        text = text.replace(' ?', '?')
        text = text.replace(' ;', ';')
        text = text.replace(' :', ':')

        # Enhanced character corrections
        char_fixes = {
            'rn': 'm', 'cl': 'd', 'li': 'h', 'vv': 'w', 'nn': 'n',
            '1l': 'll', 'il': 'll', 'I1': 'll', 'oO': 'oo',
            '0': 'o', '1': 'l', '5': 's', '8': 'b', '6': 'b'
        }

        # Apply fixes carefully to preserve intentional characters
        words = text.split()
        corrected_words = []

        for word in words:
            punct = ''
            clean_word = word

            # Extract punctuation
            if word and word[-1] in '.,!?;:()-\'\"':
                punct = word[-1]
                clean_word = word[:-1]

            # Apply character fixes
            original_word = clean_word
            for wrong, right in char_fixes.items():
                if wrong in clean_word:
                    # Only apply if it makes sense in context
                    new_word = clean_word.replace(wrong, right)
                    if len(new_word) > 0:
                        clean_word = new_word

            corrected_words.append(clean_word + punct)

        result = ' '.join(corrected_words).lower()

        # Final cleanup
        result = result.replace('  ', ' ').strip()

        return result

    def extract_text_hybrid(self, img):
        """Enhanced hybrid approach with smart fallback"""
        # Primary: TrOCR
        trocr_text = self.extract_text_with_trocr(img)

        if trocr_text and len(trocr_text.strip()) > 3:
            # Check quality
            quality_score = self.assess_text_quality(trocr_text)
            if quality_score > 0.6:
                return trocr_text

        # Fallback: Enhanced Tesseract
        if img is None:
            return trocr_text if trocr_text else ""

        pil_img = Image.fromarray(img)

        # Enhanced preprocessing for Tesseract
        enhancer = ImageEnhance.Contrast(pil_img)
        pil_img = enhancer.enhance(1.3)

        enhancer = ImageEnhance.Sharpness(pil_img)
        pil_img = enhancer.enhance(1.4)

        results = []
        for config_name, config in self.tesseract_configs.items():
            try:
                text = pytesseract.image_to_string(pil_img, config=config)
                text = self.clean_ocr_text(text)

                if len(text.strip()) > 2:
                    confidence = self.assess_text_quality(text)
                    results.append({'text': text, 'confidence': confidence})
            except:
                continue

        # Return best result
        if results:
            best_tesseract = max(results, key=lambda x: x['confidence'])

            # Compare TrOCR vs Tesseract
            if trocr_text:
                trocr_quality = self.assess_text_quality(trocr_text)
                if best_tesseract['confidence'] > trocr_quality * 1.2:
                    return best_tesseract['text']
                else:
                    return trocr_text
            else:
                return best_tesseract['text']

        return trocr_text if trocr_text else ""

    def assess_text_quality(self, text):
        """Assess overall text quality"""
        if not text:
            return 0

        text = text.strip()
        if len(text) < 3:
            return 0.1

        words = text.split()
        if not words:
            return 0.1

        # Multiple quality factors
        alpha_ratio = sum(1 for c in text if c.isalpha()) / len(text)
        avg_word_len = sum(len(w.strip('.,!?;:()-\'\"')) for w in words) / len(words)
        space_ratio = text.count(' ') / len(text)

        # Realistic word check
        realistic_words = 0
        for word in words:
            clean_word = word.strip('.,!?;:()-\'\"')
            if len(clean_word) >= 2:
                vowels = sum(1 for c in clean_word.lower() if c in 'aeiou')
                if vowels > 0:
                    realistic_words += 1

        word_realism = realistic_words / len(words) if words else 0

        quality = (
            alpha_ratio * 0.4 +
            min(avg_word_len / 6, 1.0) * 0.2 +
            min(space_ratio * 6, 0.2) * 0.2 +
            word_realism * 0.2
        )

        return min(1.0, quality)

    def clean_ocr_text(self, text):
        """Clean OCR output text"""
        text = ' '.join(text.split())

        # Basic corrections
        corrections = {
            '|': 'l', '~': '-', '^': '', '`': "'",
            'rn': 'm', 'cl': 'd', 'li': 'h', 'vv': 'w',
            '0': 'o', '1': 'l', '5': 's'
        }

        for wrong, right in corrections.items():
            text = text.replace(wrong, right)

        return text.lower().strip()

    def process_dataset(self, image_paths, texts, sample_size=None):
        """Process dataset with enhanced recognition"""
        print("Processing dataset with Enhanced TrOCR Recognition...")

        if sample_size:
            indices = np.random.choice(len(image_paths), min(sample_size, len(image_paths)), replace=False)
            image_paths = [image_paths[i] for i in indices]
            texts = [texts[i] for i in indices]

        results = []

        for i, (img_path, true_text) in enumerate(zip(image_paths, texts)):
            if i % 20 == 0:
                print(f"Processed {i}/{len(image_paths)} images...")

            processed_img = self.enhanced_preprocessing_for_trocr(img_path)
            predicted_text = self.extract_text_hybrid(processed_img)
            accuracy = self.calculate_accuracy(true_text, predicted_text)

            result = {
                'image_path': img_path,
                'true_text': true_text,
                'predicted_text': predicted_text,
                'character_accuracy': accuracy['character_accuracy'],
                'word_accuracy': accuracy['word_accuracy'],
                'levenshtein_distance': accuracy['levenshtein_distance']
            }

            results.append(result)

        self.results = results
        return results

    # Keep all existing accuracy calculation methods unchanged
    def calculate_accuracy(self, true_text, predicted_text):
        """Calculate accuracy metrics - KEEP UNCHANGED"""
        true_text = true_text.lower().strip()
        predicted_text = predicted_text.lower().strip()

        char_accuracy = self.lcs_accuracy(true_text, predicted_text)

        true_words = true_text.split()
        pred_words = predicted_text.split()

        if not true_words:
            word_accuracy = 1.0 if not pred_words else 0.0
        else:
            word_matches = sum(1 for tw, pw in zip(true_words, pred_words) if tw == pw)
            word_accuracy = word_matches / len(true_words)

        lev_dist = self.levenshtein_distance(true_text, predicted_text)

        return {
            'character_accuracy': char_accuracy,
            'word_accuracy': word_accuracy,
            'levenshtein_distance': lev_dist
        }

    def lcs_accuracy(self, s1, s2):
        """Calculate accuracy using Longest Common Subsequence - KEEP UNCHANGED"""
        if not s1 and not s2:
            return 1.0
        if not s1 or not s2:
            return 0.0

        m, n = len(s1), len(s2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i-1] == s2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        lcs_length = dp[m][n]
        return lcs_length / max(m, n)

    def levenshtein_distance(self, s1, s2):
        """Calculate Levenshtein distance - KEEP UNCHANGED"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def analyze_results(self):
        """Analyze and display results - KEEP UNCHANGED"""
        if not self.results:
            return

        char_accuracies = [r['character_accuracy'] for r in self.results]
        word_accuracies = [r['word_accuracy'] for r in self.results]

        self.accuracy_metrics = {
            'mean_character_accuracy': np.mean(char_accuracies),
            'mean_word_accuracy': np.mean(word_accuracies),
            'median_character_accuracy': np.median(char_accuracies),
            'total_samples': len(self.results)
        }

        print("ENHANCED TrOCR PERFORMANCE ANALYSIS")
        print("=" * 50)
        print(f"Total samples: {self.accuracy_metrics['total_samples']}")
        print(f"Mean character accuracy: {self.accuracy_metrics['mean_character_accuracy']:.4f}")
        print(f"Mean word accuracy: {self.accuracy_metrics['mean_word_accuracy']:.4f}")
        print(f"Median character accuracy: {self.accuracy_metrics['median_character_accuracy']:.4f}")

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.hist(char_accuracies, bins=30, alpha=0.7, color='blue')
        plt.xlabel('Character Accuracy')
        plt.ylabel('Frequency')
        plt.title('Character Accuracy Distribution')
        plt.axvline(np.mean(char_accuracies), color='red', linestyle='--',
                   label=f'Mean: {np.mean(char_accuracies):.3f}')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.hist(word_accuracies, bins=30, alpha=0.7, color='green')
        plt.xlabel('Word Accuracy')
        plt.ylabel('Frequency')
        plt.title('Word Accuracy Distribution')
        plt.axvline(np.mean(word_accuracies), color='red', linestyle='--',
                   label=f'Mean: {np.mean(word_accuracies):.3f}')
        plt.legend()

        plt.tight_layout()
        plt.show()

    def show_sample_results(self, num_samples=5):
        """Show sample results - KEEP UNCHANGED"""
        if not self.results:
            return

        sorted_results = sorted(self.results, key=lambda x: x['character_accuracy'], reverse=True)

        print("TOP PERFORMING SAMPLES:")
        print("=" * 50)
        for i, result in enumerate(sorted_results[:num_samples]):
            print(f"Sample {i+1}:")
            print(f"True: '{result['true_text']}'")
            print(f"Pred: '{result['predicted_text']}'")
            print(f"Char Acc: {result['character_accuracy']:.4f}")
            print(f"Word Acc: {result['word_accuracy']:.4f}")
            print("-" * 50)

# Main execution function
def main_trocr_enhanced():
    dataset_path = "./iamsentences"

    if not os.path.exists(dataset_path):
        for item in os.listdir("."):
            if "iam" in item.lower() or "sentence" in item.lower():
                dataset_path = item
                break

    # Initialize TrOCR recognizer
    recognizer = EnhancedTrOCRHandwrittenTextRecognizer(dataset_path)

    # Load data
    image_paths, texts = recognizer.load_metadata()

    if len(image_paths) == 0:
        print("No valid data found!")
        return None

    # Process with TrOCR
    sample_size = min(25, len(image_paths))  # Can handle more with TrOCR
    print(f"Processing {sample_size} images with TrOCR...")

    results = recognizer.process_dataset(image_paths, texts, sample_size=sample_size)

    # Analyze results
    recognizer.analyze_results()

    # Show samples
    recognizer.show_sample_results(num_samples=25)

    return recognizer

# Usage
print("""
Enhanced TrOCR Handwritten Text Recognizer
==========================================

Installation required:
pip install transformers torch torchvision

Run: recognizer = main_trocr_enhanced()

Key improvements:
- TrOCR models for superior handwriting recognition
- Enhanced punctuation preservation
- Hybrid approach with Tesseract fallback
- Better preprocessing for character recognition
- All original data loading functions preserved
""")

# Auto-run
if __name__ == "__main__":
    recognizer = main_trocr_enhanced()

import types
import cv2
import matplotlib.pyplot as plt
import os
import numpy as np
from IPython.display import display, clear_output
import ipywidgets as widgets

def show_sample_results_all_at_once(self, num_samples=5, show_images=True, sort_by='character_accuracy'):
    """
    Show sample results with corresponding images all at once

    Args:
        num_samples: Number of samples to display
        show_images: Whether to show images
        sort_by: 'character_accuracy', 'word_accuracy', or 'levenshtein_distance'
    """
    if not self.results:
        print("No results available to display!")
        return

    # Sort results based on the specified criterion
    reverse_sort = sort_by != 'levenshtein_distance'  # Lower is better for Levenshtein
    sorted_results = sorted(self.results, key=lambda x: x[sort_by], reverse=reverse_sort)

    total_samples = min(num_samples, len(sorted_results))

    print(f"üìä DISPLAYING {total_samples} SAMPLES (Sorted by {sort_by})")
    print("=" * 80)

    for i, result in enumerate(sorted_results[:num_samples]):
        print(f"\nüîç SAMPLE {i+1}/{total_samples}")
        print("=" * 50)

        # Display text information
        print(f"üìÅ Image File: {os.path.basename(result['image_path'])}")
        print(f"üìù True Text: '{result['true_text']}'")
        print(f"ü§ñ Predicted: '{result['predicted_text']}'")

        # Display metrics with color coding (using emojis for visual appeal)
        char_acc = result['character_accuracy']
        word_acc = result['word_accuracy']
        lev_dist = result['levenshtein_distance']

        # Color coding with emojis
        char_emoji = "üü¢" if char_acc > 0.9 else "üü°" if char_acc > 0.7 else "üî¥"
        word_emoji = "üü¢" if word_acc > 0.9 else "üü°" if word_acc > 0.7 else "üî¥"

        print(f"{char_emoji} Character Accuracy: {char_acc:.4f} ({char_acc*100:.2f}%)")
        print(f"{word_emoji} Word Accuracy: {word_acc:.4f} ({word_acc*100:.2f}%)")
        print(f"üìè Levenshtein Distance: {lev_dist}")

        # Calculate and display additional metrics
        if len(result['true_text']) > 0:
            error_rate = (1 - char_acc) * 100
            print(f"‚ùå Character Error Rate: {error_rate:.2f}%")

        if show_images:
            try:
                # Create a figure for this sample
                fig, axes = plt.subplots(1, 2, figsize=(15, 6))
                fig.suptitle(f"Sample {i+1}: {os.path.basename(result['image_path'])}", fontsize=14, fontweight='bold')

                # Load and display original image
                original_img = cv2.imread(result['image_path'], cv2.IMREAD_GRAYSCALE)
                if original_img is not None:
                    axes[0].imshow(original_img, cmap='gray')
                    axes[0].set_title(f"Original Image\nSize: {original_img.shape}", fontsize=12)
                    axes[0].axis('off')

                    # Add border based on accuracy
                    if char_acc > 0.9:
                        axes[0].add_patch(plt.Rectangle((0, 0), original_img.shape[1]-1, original_img.shape[0]-1,
                                                      fill=False, edgecolor='green', linewidth=3))
                    elif char_acc > 0.7:
                        axes[0].add_patch(plt.Rectangle((0, 0), original_img.shape[1]-1, original_img.shape[0]-1,
                                                      fill=False, edgecolor='orange', linewidth=3))
                    else:
                        axes[0].add_patch(plt.Rectangle((0, 0), original_img.shape[1]-1, original_img.shape[0]-1,
                                                      fill=False, edgecolor='red', linewidth=3))
                else:
                    axes[0].text(0.5, 0.5, '‚ùå Image not found', ha='center', va='center', fontsize=12)
                    axes[0].set_title("Original Image - Not Found", fontsize=12)
                    axes[0].axis('off')

                # Load and display processed image
                try:
                    processed_img = self.enhanced_preprocessing_for_trocr(result['image_path'])
                    if processed_img is not None:
                        axes[1].imshow(processed_img, cmap='gray')
                        axes[1].set_title(f"Processed Image\nChar Acc: {char_acc:.3f} | Word Acc: {word_acc:.3f}",
                                        fontsize=12)
                        axes[1].axis('off')

                        # Add same colored border
                        if char_acc > 0.9:
                            axes[1].add_patch(plt.Rectangle((0, 0), processed_img.shape[1]-1, processed_img.shape[0]-1,
                                                          fill=False, edgecolor='green', linewidth=3))
                        elif char_acc > 0.7:
                            axes[1].add_patch(plt.Rectangle((0, 0), processed_img.shape[1]-1, processed_img.shape[0]-1,
                                                          fill=False, edgecolor='orange', linewidth=3))
                        else:
                            axes[1].add_patch(plt.Rectangle((0, 0), processed_img.shape[1]-1, processed_img.shape[0]-1,
                                                          fill=False, edgecolor='red', linewidth=3))
                    else:
                        axes[1].text(0.5, 0.5, '‚ùå Processing failed', ha='center', va='center', fontsize=12)
                        axes[1].set_title("Processed Image - Failed", fontsize=12)
                        axes[1].axis('off')

                except Exception as e:
                    axes[1].text(0.5, 0.5, f'‚ùå Processing error:\n{str(e)[:50]}...',
                               ha='center', va='center', fontsize=10)
                    axes[1].set_title("Processed Image - Error", fontsize=12)
                    axes[1].axis('off')

                # Add text comparison at the bottom
                fig.text(0.1, 0.02, f"True: '{result['true_text']}'", fontsize=11, weight='bold', color='blue')
                fig.text(0.1, 0.01, f"Pred: '{result['predicted_text']}'", fontsize=11, weight='bold',
                        color='green' if char_acc > 0.9 else 'orange' if char_acc > 0.7 else 'red')

                plt.tight_layout()
                plt.subplots_adjust(bottom=0.1)
                plt.show()

            except Exception as e:
                print(f"‚ùå Error displaying images: {str(e)}")

        print("-" * 50)

    print(f"\n‚úÖ Completed displaying {total_samples} samples")


def show_sample_results_summary(self, num_samples=5, sort_by='character_accuracy'):
    """Show a summary table of results without images"""
    if not self.results:
        print("No results available to display!")
        return

    reverse_sort = sort_by != 'levenshtein_distance'
    sorted_results = sorted(self.results, key=lambda x: x[sort_by], reverse=reverse_sort)

    print(f"üìã RESULTS SUMMARY (Top {num_samples} by {sort_by})")
    print("=" * 100)
    print(f"{'#':<3} {'Image':<25} {'Char Acc':<10} {'Word Acc':<10} {'Lev Dist':<10} {'True Text':<20} {'Predicted':<20}")
    print("-" * 100)

    for i, result in enumerate(sorted_results[:num_samples]):
        filename = os.path.basename(result['image_path'])[:22] + "..." if len(os.path.basename(result['image_path'])) > 25 else os.path.basename(result['image_path'])
        true_text = result['true_text'][:17] + "..." if len(result['true_text']) > 20 else result['true_text']
        pred_text = result['predicted_text'][:17] + "..." if len(result['predicted_text']) > 20 else result['predicted_text']

        print(f"{i+1:<3} {filename:<25} {result['character_accuracy']:<10.4f} "
              f"{result['word_accuracy']:<10.4f} {result['levenshtein_distance']:<10} "
              f"{true_text:<20} {pred_text:<20}")


# Bind the methods to your existing recognizer instance
recognizer.show_sample_results_all_at_once = types.MethodType(show_sample_results_all_at_once, recognizer)
recognizer.show_sample_results_summary = types.MethodType(show_sample_results_summary, recognizer)

# Usage examples:
print("üöÄ New methods added to your recognizer!")
print("\nüìñ Usage:")
print("recognizer.show_sample_results_all_at_once(num_samples=5, show_images=True)")
print("recognizer.show_sample_results_summary(num_samples=10)")
print("recognizer.show_sample_results_all_at_once(num_samples=3, sort_by='word_accuracy')")

# Example usage:
# recognizer.show_sample_results_all_at_once(num_samples=5, show_images=True)
# recognizer.show_sample_results_summary(num_samples=10)
# recognizer.show_sample_results_all_at_once(num_samples=3, sort_by='word_accuracy', show_images=True)

# Display all samples at once with images
recognizer.show_sample_results_all_at_once(num_samples=5, show_images=True)

# Quick summary table
recognizer.show_sample_results_summary(num_samples=10)

# Sort by word accuracy
recognizer.show_sample_results_all_at_once(num_samples=3, sort_by='word_accuracy')

# Display without images for faster viewing
recognizer.show_sample_results_all_at_once(num_samples=10, show_images=False)







